{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISYS2407 Information Systems Solutions & Design\n",
    "\n",
    "# Discretization, One-Hot encoding and model building\n",
    "\n",
    "\n",
    "###### Â© France and Christopher Cheong 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library for pickling\n",
    "import joblib\n",
    "\n",
    "# Library needed for counting categorical values\n",
    "import collections\n",
    "\n",
    "# Also need pandas here\n",
    "import pandas as pd\n",
    "\n",
    "# Library for replacing labels with numbers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Libary needed for onehot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# Just use pandas get_dummies which is a lot easier to use\n",
    "\n",
    "# Library for splitting the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Import the model library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Import the libraries for computing the metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Import the plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Load the cleaned data\n",
    "\n",
    "#### Pickled file must exist in your folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pickled file\n",
    "diabetes_df = joblib.load('diabetes-cleaned.pkl')  \n",
    "\n",
    "# Check\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Split the data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the features in variable X (uppercase as there are multiple features)\n",
    "\n",
    "# Features are variables that affect the target/label\n",
    "# So, it's all the columns excluding the target column\n",
    "# However, you may also use a subset of features previously identified as best features\n",
    "# You might want to experiment with both the full set and the best features\n",
    "feature_cols = [\n",
    "    'num_pregnancies', \n",
    "    'glucose', \n",
    "    'blood_pressure',\n",
    "    'skin_thickness',\n",
    "    'insulin', \n",
    "    'bmi', \n",
    "    'pedigree', \n",
    "    'age'\n",
    "]\n",
    "\n",
    "X = diabetes_df[feature_cols]\n",
    "#print('X:\\n', X)\n",
    "\n",
    "# Store the labels/target in variable y (lower case as its a single value)\n",
    "y = diabetes_df['outcome']\n",
    "#print('y:\\n', y)\n",
    "\n",
    "# Split into train/test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, # keep 20% for testing\n",
    "                                                    random_state=2 # pass an int for reproducible rtesult\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Data Transformations to improve modelling performance\n",
    "\n",
    "#### Should be performed after the data has been split into training and testing sets to prevent information leakage (train-test contamination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Discretization of continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When checking the number of unique values, the blood pressure column was found to contain 47 values\n",
    "# This is evidence that it's a continuous variable\n",
    "\n",
    "# Some classification algorithms do not work well with continuous values - too many values\n",
    "# So, depending on the classification algorithm you are usind, you might need to categorise these values\n",
    "# How? Need some domain knowlege\n",
    "# For blood pressure we could use the following:\n",
    "# less than 80 = normal, between 80-89 = high, greater than 89 = very high\n",
    "\n",
    "# Define a function to classify this\n",
    "# Pass the whole dataframe as parameter\n",
    "# And return a category (a word label but you could also return numeric labels)\n",
    "# NOTE: Do not create too many categories as this will affect the learning algorithms\n",
    "def bp_category(df): \n",
    "    if df[\"blood_pressure\"] <= 80:\n",
    "        return \"bp_normal\"\n",
    "    elif (df[\"blood_pressure\"] > 80) & (df[\"blood_pressure\"] <= 89):\n",
    "        return \"bp_high\"\n",
    "    elif df[\"blood_pressure\"] > 89:\n",
    "        return \"bp_very_high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the  previously defined function to the training set\n",
    "\n",
    "# Use apply() to apply a lambda function for all the rows of the dataframe\n",
    "# The lambda function calls the previously defined function bp_category()\n",
    "# To which it passes the diabetes_df as parameter\n",
    "# And a category is returned and saved in a new column named \"blood_pressure_category\"\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.apply.html\n",
    "# the backslash is a line continuation character - there should be nothing after it, not even a space\n",
    "\n",
    "# Warning: A value is trying to be set on a copy of a slice from a DataFrame.\n",
    "# Try using .loc[row_indexer,col_indexer] = value instead\n",
    "# Below is another fix - make a copy\n",
    "X_train = X_train.copy()\n",
    "X_train[\"blood_pressure_category\"] = \\\n",
    "    X_train.apply(lambda X_train:bp_category(X_train), axis = 1) # axis=1 = row-wise\n",
    "\n",
    "# The column \"blood_pressure\" is no longer needed and hence can be deleted \n",
    "X_train.drop(['blood_pressure'], axis=1, inplace=True)\n",
    "\n",
    "# Check\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the  previously defined function to the testing set\n",
    "X_test = X_test.copy()\n",
    "X_test[\"blood_pressure_category\"] = \\\n",
    "    X_test.apply(lambda X_test:bp_category(X_test), axis = 1) # axis=1 = row-wise\n",
    "\n",
    "# The column \"blood_pressure\" is no longer needed and hence can be deleted \n",
    "X_test.drop(['blood_pressure'], axis=1, inplace=True)\n",
    "\n",
    "# Check\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 One-hot encoding of columns with multiple categories\n",
    "\n",
    "#### Note: Since one-hot encoding generates lots of dummy variables (columns), this has an impact on certain machine learning algorithms - only use when justified (need to experiment to find out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. Select the columns to one-hot encode\n",
    "# Make sure the columns are categorical columns with multiple labels \n",
    "# It does not matter whether the labels are words or numbers\n",
    "# Better list all the columns and comment out the ones you don't need\n",
    "columns_to_onehot_encode = [\n",
    "    #'num_pregnancies', \n",
    "    #'glucose', \n",
    "    #'blood_pressure', # this column no longer exists\n",
    "    'blood_pressure_category',\n",
    "    #'skin_thickness',\n",
    "    #'insulin', \n",
    "    #'bmi', \n",
    "    #'pedigree', \n",
    "    #'age'\n",
    "]\n",
    "\n",
    "# 2 Instantiate a one-hot encoder\n",
    "#enc = OneHotEncoder() # No, this will generate strings, not numbers\n",
    "enc = LabelBinarizer()\n",
    "\n",
    "# 3. Fit the encoder on the training column and transform the training and testing columns\n",
    "# Use a loop to label encode all the required columns \n",
    "for col in columns_to_onehot_encode:\n",
    "    # Option 1. Using pandas (simpler than sklearn)\n",
    "    ## Fix X_train\n",
    "    dummies_df = pd.get_dummies(X_train[col], prefix=\"bpc\") # generate dataframe of dummies\n",
    "    X_train.drop([col], axis=1, inplace=True) # drop original column\n",
    "    X_train = pd.concat([X_train, dummies_df], axis=1) # concatenate both dataframes\n",
    "    ## Fix X_test\n",
    "    dummies_df = pd.get_dummies(X_test[col], prefix=\"bpc\")\n",
    "    X_test.drop([col], axis=1, inplace=True)\n",
    "    X_test = pd.concat([X_test, dummies_df], axis=1)\n",
    "    \n",
    "    # Option 2: Using sklearn    \n",
    "    # First fit the encoder to the training data\n",
    "    #enc.fit(X_train[col].values.reshape(-1, 1)) # need to reshape\n",
    "    #X_train_array = enc.transform(X_train[col])\n",
    "    #X_test_array = enc.transform(X_test[col])\n",
    "    #print(X_train_array)\n",
    "    #print(X_train_array)\n",
    "    # numpy arrays generated - have no column names and don't know the order the codes were generated\n",
    "    # need to write complex code to generate column names for variable number of columns\n",
    "    # to be able to convert the numpy array to a daframe\n",
    "    # Better use the pandas solution!\n",
    "     \n",
    "# Check\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Other data transformations e.g. label encoding, scaling, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Initial/baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Fit initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The two main steps are:\n",
    "# 1: Instantiate model and fit on training data\n",
    "# 2: Predict using test data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance metrics of the baseline model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continue with the rest of the modelling steps - e.g."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Improved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.1 Use grid search to find best hyperameters (details in grid search notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 Fit improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Evaluate  improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Save the model for further evaluation (in another notebook)\n",
    "#### Should also save the train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Uncomment the code if you want to use this i.e. delete the triple quotes at the start and end of this cell\n",
    "\n",
    "# Pickle the model for later evaluation\n",
    "joblib.dump(lr_model, 'model-xxx-xxx.pkl')  # Use the right model name and a suitable file name\n",
    "\n",
    "# Also need to pickle the training and testing sets\n",
    "joblib.dump(X_train, 'X_train.pkl') \n",
    "joblib.dump(X_test, 'X_test.pkl') \n",
    "joblib.dump(y_train, 'y_train.pkl') \n",
    "joblib.dump(y_test, 'y_test.pkl')\n",
    "\n",
    "# Note: make sure that the model and the training/testing sets match \n",
    "# i.e. the model was built using this particular training set\n",
    "# and the testing set matches this partiular training\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
